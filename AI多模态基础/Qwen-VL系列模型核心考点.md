## 目录
1、Qwen-VL系列视觉编码器的核心演进路径及每代解决的问题？

2、Qwen2-VL中提出的M-RoPE如何统一处理文本、图像和视频的位置编码？

3、Qwen-VL系列模型的三阶段训练范式的具体运作机制与各阶段数据策略？

4、Qwen2.5-VL使用绝对位置坐标相比归一化坐标在目标检测中的优势？

5、Qwen2.5-VL使用动态FPS采样与3D patch划分如何协同提升视频理解？

6、Qwen2.5-VL后训练阶段如何结合监督微调（SFT）和直接偏好优化（DPO）？

7、Qwen-VL系列模型中多模态统一序列化格式的具体实现方式？

8、Qwen2-VL及之后模型使用的Naive Dynamic Resolution机制的原理与效果？

9、Qwen2.5-VL预训练数据构建的核心质量策略？

10、从Qwen-VL到Qwen2.5-VL的多语言支持演进？

## 1、Qwen-VL系列视觉编码器的核心演进路径及每代解决的问题？
Qwen-VL系列的视觉编码器演进，清晰地体现了团队在追求更高视觉理解精度和更低计算开销之间的权衡与创新。

**第一代Qwen-VL** 搭建了一个稳健的基线。它采用了基于OpenClip的ViT-bigG架构，但处理方式相对传统：将所有输入图像固定缩放到448x448分辨率。这样做虽然保证了处理速度，但高分辨率图像的细节信息损失是无法避免的。为了将漫长的图像特征序列适配到语言模型，它引入了一个非常关键的设计——**位置感知视觉-语言适配器**。这个适配器通过256个可学习的查询（Query），通过交叉注意力机制从图像特征中提炼出256个token的“视觉摘要”。同时，它在这个适配器中保留了2D绝对位置编码，确保了空间信息的不丢失。这个阶段的核心目标是**验证架构可行性**并建立基础的视觉-语言关联。

![](https://cdn.nlark.com/yuque/0/2025/png/42748353/1758445412047-915a81db-0481-403b-a96e-8d5194ff217d.png)

**第二代Qwen2-VL** 的核心突破是**动态分辨率支持**。模型不再对图像进行强制缩放，而是允许高分辨率图像生成更多的视觉token（最多16384个），从而保留更丰富的细节，这对于理解图表中的小字或图像中的微小物体至关重要。随之而来的问题是序列长度可能变得非常长。为此，它采用了一个轻量级的**MLP压缩器**，将相邻的2x2个视觉token合并为1个，高效地减少了序列长度。更重要的是，它在ViT中集成了**2D-RoPE**，这是一种将旋转位置编码扩展到二维空间的方法，让模型能更精确地理解像素之间的空间相对关系。这一代的核心任务是**提升高分辨率图像的细节感知能力**并**优化计算效率**。

![](https://cdn.nlark.com/yuque/0/2025/png/42748353/1758445486072-5e57d9ad-bd66-4267-916d-346f4ea6e20c.png)

**第三代Qwen2.5-VL** 的视觉编码器近乎是一次重构。它支持**原生分辨率输入**，并引入了**窗口注意力（Window Attention）** 机制，使得计算复杂度与序列长度呈线性关系而非二次增长，从而能高效处理手机全屏截图或高清海报等超高分辨率输入。对于视频模态，它创新性地使用了**3D patch划分**，将连续两帧的图像组合成一个处理单元，极大地提升了对视频时序信息的理解效率。这一代的使命是**突破分辨率和序列长度的极限**，为模型处理长视频、复杂文档等现实场景扫清架构障碍。

![](https://cdn.nlark.com/yuque/0/2025/png/42748353/1758445533211-3321b4b0-03c8-4b03-8763-1fd8168d83d1.png)



## **2、Qwen2-VL中提出的**M-RoPE如何统一处理文本、图像和视频的位置编码？
M-RoPE的核心思想是**为多模态数据构建一个统一的、分解式的时空坐标系**。

传统的RoPE是为文本这种一维序列设计的，它通过旋转矩阵为每个token赋予一个基于其顺序的位置信息。但图像是二维的，视频是三维（时间+空间）的，直接套用一维编码会丢失大量信息。

M-RoPE的解决方案是将位置嵌入分解为**时间（t）、高度（h）、宽度（w）**三个正交的分量。

+ 对于**纯文本输入**，这三个分量被赋予完全相同的值（即token的序列位置），此时M-RoPE在数学上完全等价于一维RoPE，实现了完美的向后兼容。
+ 对于**图像输入**，将“时间”分量置为一个常数（例如0），而“高度”和“宽度”分量则根据每个视觉token在图像中的具体坐标（x, y）进行赋值。这样，模型就能精确地知道一个token是来自图像的左上角还是右下角。
+ 对于**视频输入**，“时间”分量会随着帧序号的增加而递增（t=0,1,2,...），而每一帧内的空间信息依旧由高度和宽度分量编码。这就自然地同时编码了时空信息。

![](https://cdn.nlark.com/yuque/0/2025/png/42748353/1758445612693-567bbcf2-0690-4771-846d-527d066151d3.png)

这种分解式的设计，使得模型可以用同一套数学机制来理解单词在句子中的顺序、像素在图像中的位置、以及帧在视频中的顺序，是一种极其优雅和统一的解决方案。

## 3、Qwen-VL系列模型的三阶段训练范式的具体运作机制与各阶段数据策略？
Qwen-VL的三阶段训练是一个“先预训练，再解锁（**多任务预训练**），后精修（**指令微调（SFT）**）”的经典范式，后续迭代也基于此进行优化。

**第一阶段：预训练**

+ **目标**：在海量图像-文本对上，让模型学习视觉表征与语言概念的最基本关联。此时，**大型语言模型（LLM）的参数是被冻结的**，只训练视觉编码器和视觉-语言适配器。这相当于先给模型“植入”视觉神经元。
+ **数据**：规模巨大，约50亿对，但经过严格清洗后保留14亿高质量对（英语77.3%，中文22.7%），清洗保留率28%。数据来源多样，如LAION、Coyo等。

![](https://cdn.nlark.com/yuque/0/2025/png/42748353/1758445787915-b81eee20-1154-44da-8a49-23043cba23a1.png)

**第二阶段：多任务预训练**

+ **目标**：引入高质量、细粒度的标注数据，**解锁所有模型参数**，激发模型的多任务能力（如VQA、定位、OCR等）。
+ **数据**：约77M样本，任务类型多达7种，包括说明、VQA、对齐、引用对齐、接地说明、OCR、纯文本自回归。数据来源包括GRIT、RefCOCO等权威数据集和内部数据。
+ 在此阶段，图像分辨率会提升至448x448，并移除窗口注意力以捕捉更多细节。

![](https://cdn.nlark.com/yuque/0/2025/png/42748353/1758445816628-6b9a6b1b-7d37-40a6-935b-73ba55855d2e.png)

**第三阶段：指令微调（SFT）**

+ **目标**：让模型学会遵循人类指令进行对话和任务执行。此时，**视觉编码器被冻结**，只微调LLM和适配器，生成最终的Chat版本。
+ **数据**：使用约350K指令数据，包含模型自生成的描述、人工标注的定位数据、以及多轮对话数据，旨在让模型变得“有用”和“易用”。

![](https://cdn.nlark.com/yuque/0/2025/png/42748353/1758445858331-a7afe688-e9f5-4319-9644-b68c70afe82c.png)

Qwen2.5-VL在此基础上进一步细化，将预训练拆分为**视觉预训练、多模态预训练、长上下文预训练**三个阶段，并引入了**拒绝采样**等高级技术来极致化数据质量，但其核心思想仍源于此范式。



## 4、Qwen2.5-VL使用绝对位置坐标相比归一化坐标在目标检测中的优势？
核心目的是**让模型建立起对真实物理世界的尺度感知能力**。

传统的归一化坐标（将边界框坐标除以图像宽高，缩放到0-1之间）是一种与设备无关的表示方法，但它丢失了图像的绝对尺寸信息。对于模型而言，一个在1920x1080图像中归一化坐标为(0.5, 0.5)的点，和一个在224x224图像中同样为(0.5, 0.5)的点，在抽象意义上没有区别。但现实中，前者可能代表一个巨大的广告牌上的一个像素，而后者可能代表手机屏幕上的一个图标，它们的实际物理尺寸天差地别。

Qwen2.5-VL改用**绝对坐标**，即直接使用图像的真实宽高（如`[x, y, width, height] = [960, 540, 100, 50]`）来表示边界框。这样做的好处是：

1. **消除分辨率缩放偏差**：模型无需再去猜测输入图像是否被缩放或如何缩放，定位精度更高。
2. **增强现实世界理解**：模型能逐渐学习到“100像素宽”的物体在屏幕上大概有多大，从而更好地理解UI界面、文档布局等场景。
3. **简化预处理流程**：无需为了训练和推理的一致性而进行复杂的图像预处理，流程更直接。

这个改动是模型迈向更通用、更实用的智能体（Agent）应用的关键一步，因为它需要精确理解屏幕上每个UI元素的真实大小和位置才能执行点击等操作。

## 5、Qwen2.5-VL使用动态FPS采样与3D patch划分如何协同提升视频理解？
处理长视频的核心挑战在于如何在有限的序列长度内，尽可能保留完整的时序信息和每帧的空间细节。Qwen2.5-VL通过这两项技术形成了一个高效的组合拳。

**动态采样FPS**是一种**时间维度的自适应策略**。对于一段长时间的视频（例如1小时），如果每秒都采样，会产生3600帧，序列长度会爆炸。动态采样允许模型根据视频的总长度来智能调整采样率。例如，对长视频采用较低的FPS（如0.5fps），只抽取关键帧；对短视频采用较高的FPS（如2fps），保留更丰富的动作变化。这样可以确保无论视频多长，抽取的总帧数都在一个可控的范围内，优先保证**时序信息的完整性**。

**3D patch划分**则是一种**空间-时间维度的联合压缩技术**。传统的做法是将每一帧独立编码成视觉token，然后拼接起来，这样序列长度与帧数成线性增长。Qwen2.5-VL的创新在于，它将连续的两帧图像在patch层面进行组合，形成一个3D的patch立方体，然后一次性输入给ViT编码器。

![](https://cdn.nlark.com/yuque/0/2025/png/42748353/1758446033046-b53839cb-731b-4176-a41e-fdb9ba7c9c7c.png)

这样做有两个巨大优势：

1. **序列长度减半**：两帧被压缩成一组特征，极大减少了输入后续语言模型的视觉token数量。
2. **原生时序建模**：ViT在编码的最底层就能同时看到相邻两帧的信息，从而更早、更有效地捕捉到帧与帧之间的微小变化（即运动信息）。

**协同效应**：动态采样FPS负责控制输入ViT的帧数量（T维度），而3D patch划分负责压缩每两帧产生的token数量（H*W维度）。两者共同作用，使得模型能够以前所未有的效率处理长达数十分钟甚至小时级的视频，并同时理解其中的空间细节和复杂的时间动态。

## 6、Qwen2.5-VL后训练阶段如何结合监督微调（SFT）和直接偏好优化（DPO）？
Qwen2.5-VL的后训练采用了一种非常精细的双阶段优化范式，旨在先提升能力，再对齐偏好。整个后训练阶段，视觉编码器的参数都是被冻结的。

首先在**监督微调（SFT）阶段**，核心任务是使用约200万条高质量的指令数据来充分激发模型在预训练中获得的各种能力。这批数据是精心配比的，包含50%的纯文本数据和50%的多模态（图文和视频文本）数据。为了保证数据质量，引入了一个基于Qwen2-VL的分类模型（Qwen2-VL-Instag）对海量候选数据进行智能分类和过滤。

最关键的一步是采用了**拒绝采样（Rejection Sampling）** 技术。具体来说，使用一个中间版本的Qwen2.5-VL模型，对一批带有标准答案的数据集（有Ground Truth）进行推理，生成模型的响应。然后，将模型的输出与标准答案进行严格比对，**只保留那些模型输出与正确答案匹配的样本**，而自动丢弃那些生成错误、冗长、存在代码切换或重复模式的低质量样本。

这个过程就像一个极其严格的“考官”，只有交出满分答卷的样本才能进入最终的高质量SFT数据集。通过这种方式，确保了用于微调的数据都是最高效、最准确的“教学材料”，从而让模型的性能提升事半功倍。

在随后的**直接偏好优化（DPO）阶段**，目标从“提升能力”转向“修正行为”，即让模型的输出更符合人类的偏好和价值观。使用人工标注的偏好数据（例如，选择哪个回答更好），对模型进行训练。值得注意的是，此阶段主要使用图文和纯文本数据，暂不涉及更复杂的视频模态。通过DPO，模型学会了输出更受欢迎、更有帮助且更安全的回答，完成了从“能力强大”到“行为友善”的转变。



## 7、Qwen-VL系列模型中多模态统一序列化格式的具体实现方式？
该问题触及多模态模型的核心设计哲学，模型需要处理图像、视频和文档等多种模态的输入，解决方案是采用一种**基于特殊标记的序列化格式**，将所有模态都转化为语言模型所熟悉的“token序列”，从而实现无缝的统一处理。

整个模型的处理流程可以概括为：**视觉编码器（ViT）将像素转换为视觉token，连接器（Adapter/MLP）进行压缩和投影，最终所有模态的token被拼接成一个统一的序列输入给LLM**。

具体来说：

+ **对于图像**：图像经过视觉编码器处理后产生的视觉特征序列，会被**包裹在一对特殊的标记**之中。这相当于告诉语言模型：“接下来的这一串token，是从一张图片里来的”。
+ **对于视频**：视频被视作一系列**图像帧**的集合。每一帧都会像上述图像一样被处理。同时，会在帧的特征序列前**插入时间戳标记**（如`<video t=1.5s>`），来明确指示该帧的时间位置。
+ **对于复杂文档**：为了统一处理文档中的表格、图表、公式等元素，将其全部转换为**HTML格式**的文本。HTML本身是一种文本标记语言，因此可以直接被语言模型理解和处理。图像等非文本元素则依旧用``标签嵌入到HTML文本序列中。

这种设计的好处是极致的简洁和灵活。语言模型不需要为不同模态准备不同的处理模块，它只需要学会理解这些特殊的“模态标记”和“时间戳标记”，就能以一种近乎相同的方式处理各种输入，大大降低了架构的复杂性。

## 8、Qwen2-VL及之后模型使用的Naive Dynamic Resolution机制的原理与效果？
朴素动态分辨率机制——“Naive Dynamic Resolution”是一种直观但非常有效的设计，其核心思想是**让视觉token的数量与输入图像的分辨率动态适配**，从而在计算资源允许的范围内最大限度地保留原始信息。

工作流程：

1. **动态分词**：模型不再将所有图像强制缩放到一个固定的尺寸（如224x224或448x448），而是根据图像的原生分辨率，通过视觉编码器（ViT）将其转换为相应数量的视觉token。一张1024x1024的图片自然会比一张224x224的图片产生多得多的视觉token。
2. **智能压缩**：为了避免产生的token序列过长，随后使用一个轻量的**MLP压缩器**，将**空间上相邻的2x2个**视觉token**合并为1个token**。这相当于将图像的“分辨率”在特征空间里降低了2倍，从而将序列长度减少到原来的1/4。

这个过程之所以能提升细节理解，关键在于**第一步**。对于高分辨率图像，即使经过后续的2x2合并压缩，其最终保留的视觉token数量仍然远高于低分辨率图像直接处理的结果。例如，一张4K图像最初可能生成上万token，压缩后仍能保留数千token；而一张低分辨率图像最初只能产生数百token。更多的token意味着更多的信息承载量，使得模型能够分辨出图像中更细微的元素，如文档里的小号字体、网页上的图标细节或街景中的远处路牌。

这种机制让模型在面对不同质量的输入时具备了“弹性”，既能细致入微地分析高清图片，也不会对低分辨率图片进行不必要的过度计算。

## 9、Qwen2.5-VL预训练数据构建的核心质量策略？
Qwen2.5-VL的性能飞跃，很大程度上源于在数据质量上进行的“精耕细作”。不再仅仅追求数据规模，而是通过一系列系统性的策略来构建一个“精英”数据集。

核心策略主要包括以下几个方面：

1. **交错图文数据的精细化清洗**：从海量原始数据中，通过基于CLIP模型的数据评分和一系列去重、去污的清洗流程，筛选出真正高质量、图文高度相关的样本。这确保了基础视觉-语言关联学习的可靠性。
2. **grounding数据的绝对坐标化**：正如之前提到的，在检测和定位数据中摒弃了归一化坐标，采用基于图像真实尺寸的绝对坐标。这一改动虽然微小，但让模型学习到的空间关系是基于真实世界的尺度，极大提升了其在现实应用中的泛化能力。
3. **文档数据的结构化合成**：合成了大量包含表格、图表、公式、乐谱等复杂元素的文档数据，并统一用HTML格式来标注。这种结构化的表示方式让模型能无缝地理解和推理文档中的多模态元素之间的关系。
4. **视频数据的时序精细化标注**：对于视频数据，不仅动态采样帧，还构建了详细的长视频标题描述，并以“时分秒帧”（hmsf）的格式精确标注时间戳，让模型能够建立精确的时序理解。
5. **智能体数据的多维构建**：为了训练模型成为屏幕智能体，收集了移动端、Web和桌面的截图，并利用合成引擎生成了对UI元素的精准接地（grounding）注释。这使得模型能理解“可点击的按钮”在屏幕上的具体位置。

所有这些策略的共同点在于：**不仅提供数据，更提供数据的精确上下文和结构化信息**。是在为模型构建一个标注清晰、结构严谨的“教科书”，而不仅仅是提供一堆“阅读材料”。

## 10、从Qwen-VL到Qwen2.5-VL的多语言支持演进？
多语言能力的扩展是一个系统工程，从数据和模型两个层面稳步推进。

1. **多语言OCR数据的系统整合**：广泛收集和整合了来自不同来源的OCR数据，包括合成数据、开源数据（如SROIE）和内部收集的数据，并重点覆盖了日语、韩语、阿拉伯语等更多语言。这教会了模型如何“阅读”全世界的文字。
2. **多语言指令微调数据的构建**：在SFT阶段，注入了包含多种语言的问答对和指令数据。这使得模型不仅能“看”懂多语言文字，还能用相应的语言进行思考和回答，实现了端到端的多模态多语言对话。
3. **绝对坐标的间接增益**：之前提到的绝对坐标标注策略，同样惠及多语言场景。例如，一个中文网页和一个阿拉伯文网页（从右向左书写）的UI元素位置分布可能不同，绝对坐标能帮助模型更好地理解这种与文化或语言书写方向相关的布局差异，从而做出更准确的定位。

通过这些努力，Qwen2.5-VL不再是一个仅精通中英文的专家，而是一个具备了更广泛语言视野的“多语言多模态通才”，能够更好地服务于全球化的应用场景。

