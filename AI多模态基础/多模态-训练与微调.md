## 目录

- [1.为什么多模态模型需要大规模预训练？](#1.为什么多模态模型需要大规模预训练？)
- [2.简述预训练基础模型的四种主要学习机制，并分析它们各自的优缺点](#2.简述预训练基础模型的四种主要学习机制，并分析它们各自的优缺点)
- [3.比较提示学习和微调两种方法，并分析它们在下游任务中的适用场景](#3.比较提示学习和微调两种方法，并分析它们在下游任务中的适用场景)
- [4.解释什么是思维链?](#4.解释什么是思维链?)
- [5.分析RLHF基于人类反馈的强化学习方法的原理，并阐述其优缺点?](#5.分析RLHF基于人类反馈的强化学习方法的原理，并阐述其优缺点?)
- [6.解释上下文学习在多模态大模型中的原理，并举例说明其在数学推理任务中的应用?](#6.解释上下文学习在多模态大模型中的原理，并举例说明其在数学推理任务中的应用?)
- [7.思维树如何通过图结构实现更强大的提示?](#7.思维树如何通过图结构实现更强大的提示?)

<h2 id="1.为什么多模态模型需要大规模预训练？">1.为什么多模态模型需要大规模预训练？</h2>

多模态模型需要大规模预训练的原因包括：

(1)数据丰富性：大规模预训练可以暴露模型于丰富的数据，提升其泛化能力。

(2)特征提取：通过预训练，模型能够学习到有效的特征表示，提升后续任务的表现。

(3)知识积累：预训练过程使模型积累了大量的先验知识，有助于更好地理解和处理复杂任务。

<h2 id="2.简述预训练基础模型的四种主要学习机制，并分析它们各自的优缺点">2.简述预训练基础模型的四种主要学习机制，并分析它们各自的优缺点</h2>

预训练基础模型的四种主要学习机制包括：

有监督学习：使用标注数据进行训练，模型需要学习输入和输出之间的映射关系。

半监督学习：使用少量标注数据和大量未标注数据进行训练，模型需要学习输入和输出之间的潜在关系。

弱监督学习：使用少量标注数据和大量未标注数据进行训练，但标注数据仅提供部分信息，例如图像中的某些区域或文本中的某些单词。

自监督学习：使用未标注数据进行训练，模型需要学习数据中的内在结构，例如图像中的纹理或文本中的语法规则。

<h2 id="3.比较提示学习和微调两种方法，并分析它们在下游任务中的适用场景">3.比较提示学习和微调两种方法，并分析它们在下游任务中的适用场景</h2>

提示学习通过设计特定的输入提示来引导模型生成所需的输出，它适用于模型已经具备一定泛化能力且需要生成特定类型输出的任务。

提示方法试图通过学习一个语言模型来模拟文本 x自身的概率 P(x; θ)，并使用这个概率预测 y，从而减少对大量标记数据集的需求。提示方法最基本的数学描述包含许多关于提示的研究，并且可以扩展到其他方法。

微调则是通过在下游任务的数据集上调整模型的参数来优化模型在特定任务上的性能，适用于模型在特定任务上需要显著提升性能的情况。

![](./imgs/微调.png)

（ 1) 适配器微调：这种方法使用神经适配器或模块化组件，以增强LLM 在特定领域任务上的性能，而不对 LLM 的内部参数进行大幅修改。这些适配器通常集成到现有的 LLM 结构中，允许进行特定任务的学习，同时保持原始模型基本完整。
（ 2) 任务导向微调：这种方法侧重于修改 LLM 的内部参数，以与特定任务对齐。然而，由于硬件限制和潜在的性能下降，完全更新 LLM 的所有参数是不现实的。因此，研究人员面临的挑战在于在广泛的参数空间内确定哪些参数需要被修改，或者如何高效地更新这些参数的子集。



<h2 id="4.解释什么是思维链?">4.解释什么是思维链?</h2>

传统大模型基于“预训练 + 微调”的模式始终没办法很好地完成多步骤推理任务。而 2022年 Jason Wei 提出的思维链提示（Chain-of-Thought Prompting， CoT Prompting）可以显著提高 LLM 的性能，思维链提示使 LLM 能够处理复杂的算术、常识和符号推理任务。

思维链探索了语言模型在推理任务上进行少量示范提示的能力，给定了一个由三元组组成的提示： h 输入，思维链，输出i。思维链是一系列中间的自然语言推理步骤。思维链关注 LLM 如何通过少量任务相关的自然语言数据示例进行学习。

![](./imgs/思维链.png)



<h2 id="5.分析RLHF基于人类反馈的强化学习方法的原理，并阐述其优缺点?">5.分析RLHF基于人类反馈的强化学习方法的原理，并阐述其优缺点?</h2>

OpenAI 推出的 ChatGPT 掀起了新的 AI 热潮,这一工作的背后是 LLM 生成领域的新训练范式 RLHF，即以强化学习方式依据人类反馈优化语言模型。RLHF 把训练过程分解为三个核心步骤。（1）预训练一个语言模型。（2）收集数据并训练奖励模型。（3）用强化学习的方式微调语言模型。

![](./imgs/RLHF1.png)

奖励函数的目的是将所有模型集成到一个 RLHF 流程中。给定数据集中的提示 x 和基于当前微调策略生成的文本 y，将该文本与原始提示连接，将其传递给偏好模型，该模型返回一个“可取性”的标量 rθ。此外，将来自强化学习策略的每个标记的概率分布与初始模型的概率分布进行比较，以计算它们之间差异的惩罚。

![](./imgs/RLHF2.png)

缺点：RLHF通过人类反馈来优化语言模型的输出，但需要大量高质量的人类标注数据。

<h2 id="6.解释上下文学习在多模态大模型中的原理，并举例说明其在数学推理任务中的应用?">6.解释上下文学习在多模态大模型中的原理，并举例说明其在数学推理任务中的应用?</h2>

上下文学习 是指模型能够根据输入的上下文信息来调整其输出的能力。在多模态大模型中，上下文学习原理如下：
●多模态融合：模型能够同时处理文本、图像等多种模态的信息，并将这些信息融合在一起以形成更丰富的上下文表示。
●动态调整：根据输入的上下文信息，模型能够动态地调整其内部参数和输出策略，以适应不同的任务和场景。
数学推理任务中的应用：在数学推理任务中，上下文学习可以帮助模型理解问题的背景和已知条件，从而更准确地推导出答案。例如，当模型遇到一个涉及几何图形的问题时，它可以利用图像模态的信息来辅助理解问题，并结合文本模态的信息进行推理和解答。


<h2 id="7.思维树如何通过图结构实现更强大的提示?">7.思维树如何通过图结构实现更强大的提示?</h2>

思维树 是一种通过图结构来组织信息的提示方法，其实现方式如下：
●节点表示：思维树的每个节点代表一个概念、事实或想法，节点之间通过边相连以表示它们之间的关系。
●层次化结构：思维树采用层次化的结构来组织信息，使得用户可以从宏观到微观逐步深入地思考问题。
●动态更新：用户可以根据需要动态地添加、删除或修改思维树中的节点和边，以适应不同的思考过程和需求。
通过这种图结构，思维树能够提供更丰富、更灵活的提示信息，帮助用户更好地理解和解决问题。

