## 目录

- [1.为什么多模态模型需要大规模预训练？](#1.为什么多模态模型需要大规模预训练？)
- [2.简述预训练基础模型的四种主要学习机制，并分析它们各自的优缺点](#2.简述预训练基础模型的四种主要学习机制，并分析它们各自的优缺点)
- [3.比较提示学习和微调两种方法，并分析它们在下游任务中的适用场景](#3.比较提示学习和微调两种方法，并分析它们在下游任务中的适用场景)
- [4.解释什么是思维链?](#4.解释什么是思维链?)
- [5.分析RLHF基于人类反馈的强化学习方法的原理，并阐述其优缺点?](#5.分析RLHF基于人类反馈的强化学习方法的原理，并阐述其优缺点?)
- [6.解释上下文学习在多模态大模型中的原理，并举例说明其在数学推理任务中的应用?](#6.解释上下文学习在多模态大模型中的原理，并举例说明其在数学推理任务中的应用?)
- [7.思维树如何通过图结构实现更强大的提示?](#7.思维树如何通过图结构实现更强大的提示?)
- [8.为什么LoRA不会增加额外的推理延迟？](#8.为什么LoRA不会增加额外的推理延迟？)
- [9.多模态大模型常用微调方法LoRA和Ptuning的原理，与传统fine-tuning微调有何不同?](#9.多模态大模型常用微调方法LoRA和Ptuning的原理，与传统fine-tuning微调有何不同?)
- [10.多模态训练中，LoRA的矩阵怎么初始化？为什么要初始化为全0？](#10.多模态训练中，LoRA的矩阵怎么初始化？为什么要初始化为全0？)
- [11.详细说明多模态大模型中LoRA的推理过程？](#11.详细说明多模态大模型中LoRA的推理过程？)
- [12.LoRA微调方法为什么能加速训练？](#12.LoRA微调方法为什么能加速训练？)
- [13.LoRA应该作用于Transformer的哪个参数矩阵？](#13.LoRA应该作用于Transformer的哪个参数矩阵？)
- [14.LoRA中的Rank和Alpha参数如何选取？](#14.LoRA中的Rank和Alpha参数如何选取？)

<h2 id="1.为什么多模态模型需要大规模预训练？">1.为什么多模态模型需要大规模预训练？</h2>

多模态模型需要大规模预训练的原因包括：

(1)数据丰富性：大规模预训练可以暴露模型于丰富的数据，提升其泛化能力。

(2)特征提取：通过预训练，模型能够学习到有效的特征表示，提升后续任务的表现。

(3)知识积累：预训练过程使模型积累了大量的先验知识，有助于更好地理解和处理复杂任务。

<h2 id="2.简述预训练基础模型的四种主要学习机制，并分析它们各自的优缺点">2.简述预训练基础模型的四种主要学习机制，并分析它们各自的优缺点</h2>

预训练基础模型的四种主要学习机制包括：

有监督学习：使用标注数据进行训练，模型需要学习输入和输出之间的映射关系。

半监督学习：使用少量标注数据和大量未标注数据进行训练，模型需要学习输入和输出之间的潜在关系。

弱监督学习：使用少量标注数据和大量未标注数据进行训练，但标注数据仅提供部分信息，例如图像中的某些区域或文本中的某些单词。

自监督学习：使用未标注数据进行训练，模型需要学习数据中的内在结构，例如图像中的纹理或文本中的语法规则。

<h2 id="3.比较提示学习和微调两种方法，并分析它们在下游任务中的适用场景">3.比较提示学习和微调两种方法，并分析它们在下游任务中的适用场景</h2>

提示学习通过设计特定的输入提示来引导模型生成所需的输出，它适用于模型已经具备一定泛化能力且需要生成特定类型输出的任务。

提示方法试图通过学习一个语言模型来模拟文本 x自身的概率 P(x; θ)，并使用这个概率预测 y，从而减少对大量标记数据集的需求。提示方法最基本的数学描述包含许多关于提示的研究，并且可以扩展到其他方法。

微调则是通过在下游任务的数据集上调整模型的参数来优化模型在特定任务上的性能，适用于模型在特定任务上需要显著提升性能的情况。

![](./imgs/微调.png)

（ 1) 适配器微调：这种方法使用神经适配器或模块化组件，以增强LLM 在特定领域任务上的性能，而不对 LLM 的内部参数进行大幅修改。这些适配器通常集成到现有的 LLM 结构中，允许进行特定任务的学习，同时保持原始模型基本完整。
（ 2) 任务导向微调：这种方法侧重于修改 LLM 的内部参数，以与特定任务对齐。然而，由于硬件限制和潜在的性能下降，完全更新 LLM 的所有参数是不现实的。因此，研究人员面临的挑战在于在广泛的参数空间内确定哪些参数需要被修改，或者如何高效地更新这些参数的子集。



<h2 id="4.解释什么是思维链?">4.解释什么是思维链?</h2>

传统大模型基于“预训练 + 微调”的模式始终没办法很好地完成多步骤推理任务。而 2022年 Jason Wei 提出的思维链提示（Chain-of-Thought Prompting， CoT Prompting）可以显著提高 LLM 的性能，思维链提示使 LLM 能够处理复杂的算术、常识和符号推理任务。

思维链探索了语言模型在推理任务上进行少量示范提示的能力，给定了一个由三元组组成的提示： h 输入，思维链，输出i。思维链是一系列中间的自然语言推理步骤。思维链关注 LLM 如何通过少量任务相关的自然语言数据示例进行学习。

![](./imgs/思维链.png)



<h2 id="5.分析RLHF基于人类反馈的强化学习方法的原理，并阐述其优缺点?">5.分析RLHF基于人类反馈的强化学习方法的原理，并阐述其优缺点?</h2>

OpenAI 推出的 ChatGPT 掀起了新的 AI 热潮,这一工作的背后是 LLM 生成领域的新训练范式 RLHF，即以强化学习方式依据人类反馈优化语言模型。RLHF 把训练过程分解为三个核心步骤。（1）预训练一个语言模型。（2）收集数据并训练奖励模型。（3）用强化学习的方式微调语言模型。

![](./imgs/RLHF1.png)

奖励函数的目的是将所有模型集成到一个 RLHF 流程中。给定数据集中的提示 x 和基于当前微调策略生成的文本 y，将该文本与原始提示连接，将其传递给偏好模型，该模型返回一个“可取性”的标量 rθ。此外，将来自强化学习策略的每个标记的概率分布与初始模型的概率分布进行比较，以计算它们之间差异的惩罚。

![](./imgs/RLHF2.png)

缺点：RLHF通过人类反馈来优化语言模型的输出，但需要大量高质量的人类标注数据。

<h2 id="6.解释上下文学习在多模态大模型中的原理，并举例说明其在数学推理任务中的应用?">6.解释上下文学习在多模态大模型中的原理，并举例说明其在数学推理任务中的应用?</h2>

上下文学习 是指模型能够根据输入的上下文信息来调整其输出的能力。在多模态大模型中，上下文学习原理如下：
●多模态融合：模型能够同时处理文本、图像等多种模态的信息，并将这些信息融合在一起以形成更丰富的上下文表示。
●动态调整：根据输入的上下文信息，模型能够动态地调整其内部参数和输出策略，以适应不同的任务和场景。
数学推理任务中的应用：在数学推理任务中，上下文学习可以帮助模型理解问题的背景和已知条件，从而更准确地推导出答案。例如，当模型遇到一个涉及几何图形的问题时，它可以利用图像模态的信息来辅助理解问题，并结合文本模态的信息进行推理和解答。


<h2 id="7.思维树如何通过图结构实现更强大的提示?">7.思维树如何通过图结构实现更强大的提示?</h2>

思维树 是一种通过图结构来组织信息的提示方法，其实现方式如下：
●节点表示：思维树的每个节点代表一个概念、事实或想法，节点之间通过边相连以表示它们之间的关系。
●层次化结构：思维树采用层次化的结构来组织信息，使得用户可以从宏观到微观逐步深入地思考问题。
●动态更新：用户可以根据需要动态地添加、删除或修改思维树中的节点和边，以适应不同的思考过程和需求。
通过这种图结构，思维树能够提供更丰富、更灵活的提示信息，帮助用户更好地理解和解决问题。

<h2 id="8.为什么LoRA不会增加额外的推理延迟？">8.为什么LoRA不会增加额外的推理延迟？</h2>

1、参数高效性: LoRA 不直接修改原始模型的参数，而是添加少量可训练的低秩矩阵 (通常只有几千个参数) 到目标模块中。这些新增的参数被称为 “LoRA 矩阵”。由于参数数量非常少，因此不会对模型的计算量造成显著影响。
2、推理时仅使用新增参数: 在推理过程中，LoRA 矩阵会与原始模型参数进行融合，然后进行计算。这意味着模型在推理时只需要使用原始模型参数和 LoRA 矩阵，而不需要加载和计算所有原始模型参数，从而提高了推理效率。
3、并行计算: LoRA 矩阵可以被并行地加载和计算，进一步提高了推理速度。
4、目标模块的选择: LoRA 通常只针对模型中的一些关键模块进行微调，例如注意力机制中的 query、key、value 投影矩阵。这些模块是模型中最为重要的部分，微调它们可以显著提高模型性能，而不会影响其他模块的性能。

<h2 id="9.多模态大模型常用微调方法LoRA和Ptuning的原理，与传统fine-tuning微调有何不同?">9.多模态大模型常用微调方法LoRA和Ptuning的原理，与传统fine-tuning微调有何不同?</h2>

在大模型微调领域，LoRA（Low-Rank Adaptation）和Ptuning（Prompt Tuning）是两种新兴的方法，它们旨在通过减少参数更新的数量来提高微调的效率和效果。这些方法与传统fine-tuning微调在原理和实现上有着显著的不同。

**LoRA微调原理**

LoRA通过低秩分解技术，仅更新模型参数的一个低秩近似，从而减少微调所需的计算资源和存储需求。具体来说，LoRA在预训练模型的某些层添加可训练的适配器，这些适配器是具有可学习参数的小型神经网络。在微调阶段，LoRA保持预训练模型的主要参数不变，只优化这些适配器的参数。

**Ptuning微调原理**

Ptuning，即Prompt Tuning，是一种通过修改输入提示（prompt）来微调大型预训练语言模型的方法。它不直接修改模型本身的参数，而是通过优化输入提示的嵌入向量，使模型能够生成与任务相关的输出。这种方法的优势在于其灵活性，可以快速适应不同的任务，而无需对模型架构进行任何修改。

**传统fine-tuning微调原理**

传统fine-tuning微调方法涉及全面调整模型的所有参数，以使其适应特定的下游任务。这种方法虽然能够充分利用预训练模型的通用特征，但通常需要大量的计算资源和时间，尤其是在处理大规模模型时。

**LoRA和Ptuning与传统fine-tuning的区别**

- **参数更新数量**：LoRA和Ptuning都旨在减少参数更新的数量。LoRA通过低秩分解技术更新少量参数，而Ptuning仅优化输入提示的嵌入向量。相比之下，传统fine-tuning需要更新模型中的大部分或全部参数。

- **计算资源和存储需求**：由于LoRA和Ptuning仅更新少量参数，它们通常需要更少的计算资源和存储空间。传统fine-tuning则需要更多的计算资源，尤其是在处理大规模模型时。

- **模型性能**：LoRA和Ptuning旨在在不牺牲模型性能的前提下，减少微调的计算负担。传统fine-tuning可能会导致模型过拟合，尤其是在训练数据有限的情况下。

LoRA和Ptuning通过其独特的微调策略，为大型模型的微调提供了新的解决方案，特别是在计算资源和时间有限的情况下。然而，选择哪种微调方法应根据具体任务需求、资源限制和期望的灵活性来决定。

<h2 id="10.多模态训练中，LoRA的矩阵怎么初始化？为什么要初始化为全0？">10.多模态训练中，LoRA的矩阵怎么初始化？为什么要初始化为全0？</h2>

在多模态训练中，LoRA（Low-Rank Adaptation）的矩阵初始化是一个关键步骤，它直接影响到微调的效果和收敛速度。以下是对LoRA矩阵初始化方法的详细解释，以及为什么选择初始化为全0的原因。

**LoRA矩阵初始化方法**
  
矩阵B被初始化为0，而矩阵A正常高斯初始化。 

如果B，A全都初始化为0，那么缺点与深度网络全0初始化一样，很容易导致梯度消失(因为此时初始所有神经元的功能都是等价的)。 

如果B，A全部高斯初始化，那么在网络训练刚开始就会有概率为得到一个过大的偏移值Δ W 从而引入太多噪声，导致难以收敛。 

因此，一部分初始为0，一部分正常初始化是为了在训练开始时维持网络的原有输出(初始偏移为0)，但同时也保证在真正开始学习后能够更好的收敛。

<h2 id="11.详细说明多模态大模型中LoRA的推理过程？">11.详细说明多模态大模型中LoRA的推理过程？</h2>

LoRA（Low-Rank Adaptation）是一种用于大型多模态模型的微调技术，它通过低秩分解来减少需要更新的参数数量，从而在保持模型性能的同时降低计算成本和存储需求。以下是LoRA在多模态大模型中的推理过程：

**LoRA的推理过程**

1. **冻结预训练模型参数**：
   - 在微调过程中，LoRA首先冻结预训练模型的所有参数，这些参数在微调过程中保持不变。这样做可以确保模型的基本结构和特征不受影响，同时减少需要更新的参数数量。

2. **引入低秩分解矩阵**：
   - 在Transformer架构的每一层中，LoRA注入两个可训练的低秩分解矩阵A和B。矩阵A使用随机高斯分布进行初始化，维度为r×k，负责将输入数据维度降至r维；矩阵B使用0进行初始化，维度为d×r，负责将数据维度升至d维。

3. **低秩矩阵的初始化和更新**：
   - 在微调过程中，仅训练低秩分解矩阵A和B。具体来说，待更新的参数矩阵ΔW表示为低秩分解BABA的形式，其中BB和AA是随机初始化并在微调过程中更新的。

4. **合并训练参数**：
   - 最后，将LoRA训练得到的参数与原始模型参数合并，并保存到新的模型中。这样，模型在保持原有性能的同时，获得了针对特定任务的微调效果。

**LoRA的优势**

- **减少计算资源和存储需求**：通过仅更新低秩矩阵，LoRA显著减少了微调所需的计算资源和存储空间。
- **保持模型性能**：LoRA通过低秩分解技术，能够在保持模型原始性能的基础上，实现有效的微调。
- **推理效率高**：LoRA在推理阶段不引入额外的计算量，保持了高效的推理性能。

LoRA通过其独特的低秩分解技术，为多模态大模型的微调提供了一种高效且有效的方法。它不仅减少了计算资源和存储需求，还能在保持模型性能的同时，实现快速的微调和推理。

<h2 id="12.LoRA微调方法为什么能加速训练？">12.LoRA微调方法为什么能加速训练？</h2>

1）只更新了部分参数：比如LoRA原论文就选择只更新Self Attention的参数，实际使用时我们还可以选择只更新部分层的参数；

2）减少了通信时间：由于更新的参数量变少了，所以（尤其是多卡训练时）要传输的数据量也变少了，从而减少了传输时间； 

3）采用了各种低精度加速技术，如FP16、FP8或者INT8量化等。

这三部分原因确实能加快训练速度，然而它们并不是LoRA所独有的，事实上几乎都有参数高效方法都具有这些特点。LoRA的优点是它的低秩分解很直观，在不少场景下跟全量微调的效果一致，以及在预测阶段不增加推理成本。

<h2 id="13.LoRA应该作用于Transformer的哪个参数矩阵？">13.LoRA应该作用于Transformer的哪个参数矩阵？</h2>

![image](https://github.com/user-attachments/assets/b34806ab-b3dd-40db-bfa6-f49d30b0eeac)

从上图我们可以看到： 

1）将所有微调参数都放到attention的某一个参数矩阵的效果并不好，将可微调参数平均分配到 Wq 和 Wk 的效果最好；

2）即使是秩仅取4也能在 ∆W 中获得足够的信息。

因此在实际操作中，应当将可微调参数分配到多种类型权重矩阵中，而不应该用更大的秩单独微调某种类型的权重矩阵。

<h2 id="14.LoRA中的Rank和Alpha参数如何选取？">14.LoRA中的Rank和Alpha参数如何选取？</h2>

Rank的取值比较常见的是8，理论上说Rank在4-8之间效果最好，再高并没有效果提升。不过论文的实验是面向下游单一监督任务的，因此在指令微调上根据指令分布的广度，Rank选择还是需要在8以上的取值进行测试。

alpha其实是个缩放参数，本质和learning rate相同，所以为了简化可以默认让alpha=rank，只调整lr，这样可以简化超参。
