# 目录

## 第一章 DiT核心基础知识高频考点

- [1.介绍一下DiT模型的基本概念](#1.介绍一下DiT模型的基本概念)
- [2.DiT输入图像的Patch化过程是什么样的？](#2.DiT输入图像的Patch化过程是什么样的？)
- [3.介绍一下FasterDiT](#3.介绍一下FasterDiT)
- [4.介绍一下SiT](#4.介绍一下SiT)
- [5.介绍一下LightningDiT](#5.介绍一下LightningDiT)


## 第二章 VAE核心基础知识高频考点

- [1.VAE和AE的区别是什么？](#1.VAE和AE的区别是什么？)


## 第一章 DiT核心基础知识高频考点

<h2 id="1.介绍一下DiT模型的基本概念">1.介绍一下DiT模型的基本概念</h2>

DiT（Diffusion Transformer）模型由Meta在2022年首次提出，**其主要是在ViT（Vision Transformer）的架构上进行了优化设计得到的**。**DiT是基于Transformer架构的扩散模型，将扩散模型中经典的U-Net架构完全替换成了Transformer架构**。

同时DiT是一个可扩展的架构，**DiT不仅证明了Transformer思想与扩散模型结合的有效性，并且还验证了Transformer架构在扩散模型上具备较强的Scaling能力**，在稳步增大DiT模型参数量与增强数据质量时，DiT的生成性能稳步提升。其中最大的DiT-XL/2模型在ImageNet 256x256的类别条件生成上达到了当时的SOTA（FID为2.27）性能。

DiT的整体框架并没有采用常规的Pixel Diffusion（像素扩散）架构，而是使用和Stable Diffusion相同的Latent Diffusion（潜变量扩散）架构。

为了获得图像的Latent Feature，所以DiT使用了和SD一样的VAE（基于KL-f8）模型。当我们输入512x512x3的图像时，通过VAE能够压缩生成64x64x4分辨率的Latent特征，这极大地降低了扩散模型的计算复杂度（减少Transformer的token的数量）。

同时，DiT扩散过程的nosie scheduler采用简单的Linear scheduler（timesteps=1000，beta_start=0.0001，beta_end=0.02），这与SD模型是不同的。在SD模型中，所采用的noise scheduler通常是Scaled Linear scheduler。


<h2 id="2.DiT输入图像的Patch化过程是什么样的？">2.DiT输入图像的Patch化过程是什么样的？</h2>

DiT和ViT一样，首先采用一个Patch Embedding来**将输入图像Patch化，主要作用是将VAE编码后的二维特征转化为一维序列，从而得到一系列的图像tokens**，具体如下图所示：

![ViT模型架构示意图](./imgs/ViT模型架构示意图.jpg)

同时，DiT在这个图像Patch化的过程中，设计了patch size这个超参数，它直接决定了图像tokens的大小和数量，从而影响DiT模型的整体计算量。DiT论文中共设置了三种patch size，分别是 $p = 2,4,8$ 。同时和其他Transformers模型一样，在得到图像tokens后，还要加上Positional Embeddings进行位置标记，DiT中采用经典的非学习sin&cosine位置编码技术。具体流程如下图所示：

![DiT中输入图像Patch化的示意图](./imgs/DiT中输入图像Patch化的示意图.png)

输入图像在经过VAE编码器处理后，生成一个Latent特征，我们假设其尺寸为 $I \times I \times C$，其中 $I$ 是Latent特征的宽度或高度， $C$ 是Latent特征的通道数。

接下来，用我们设定的patch size来将Latent特征进行Patch化，假设我们设定 $p = 16$ ，那么这时每个patch的尺寸为 $p \times p$ 。

由于Latent特征的尺寸是 $I \times I$ ，因此在宽度和高度方向可以分别划分出 $\frac{I}{P}$ 个patch。因此，整个Latent特征可以被分成 $\frac{I}{P}$ 个patch。

最后我们将生成的每个尺寸为 $p \times p$ 的patch展平（flatten）成一个向量，其尺寸为 $[1,p\times p\times C]$ ，这些向量就构成了DiT模型的输入tokens，总的来说，生成的token数量为：

$$T = \left(\frac{I}{p}\right)^2 $$

同时每个token的维度为 $d$ ，这是DiT输入的Latent空间维度。

如果我们设置的patch大小较小，那么生成的tokens数量就会较多，这时DiT的输入序列长度会变长，这会增加整体的计算复杂度。


<h2 id="3.介绍一下FasterDiT">3.介绍一下FasterDiT</h2>

FasterDiT 通过两个主要贡献显著加速了扩散变换器（DiT）的训练过程：

1. **SNR 概率密度函数 (PDF) 视角**：FasterDiT 扩展了传统的信号噪声比（SNR）定义，提出通过 SNR 的概率密度函数来分析训练中的数据稳健性。通过对不同训练策略下的 SNR 分布进行分析，可以更直观地理解哪些策略更适合不同的数据，从而优化训练过程，提高训练效率。

2. **新的监督方法**：FasterDiT 引入了结合速度预测和方向监督的监督方法。与传统的噪声预测不同，FasterDiT 预测噪声到数据转化的速度，并且通过余弦相似度监督速度的方向。这种方法加速了模型的训练，使其更快地收敛，并提高了生成效果。

   ![image-20250323205036349](./imgs/Faster_DiT.png)


<h2 id="4.介绍一下SiT">4.介绍一下SiT</h2>

SiT (Scalable Interpolant Transformers) 是一种新型生成模型框架，建立在扩散变换器 (DiT) 的基础上，但引入了更灵活的插值架构，使其在生成高质量图像方面表现更佳。

## 核心特点

1. SiT的核心创新在于重新思考了生成模型中的插值过程。传统扩散模型通常使用固定的前向过程，将数据分布逐步转化为高斯噪声，而SiT则提出了更为灵活的插值框架：

   1. 模块化的设计选择：SiT系统地研究了四个关键组件对生成质量的影响：
      - 时间离散化策略（连续或离散时间）
      - 模型预测方式（速度场或分数预测）
      - 插值函数选择（如Linear或GVP）
      - 采样方法（确定性ODE或随机SDE）
   2. **解耦的扩散系数**：SiT创新性地将扩散系数从前向过程中分离出来，使其可以在推理阶段独立调整，从而更精确地控制KL散度上界，提高生成质量。
   3. **降低传输成本**：实验表明，SiT的插值方式显著降低了路径长度（传输成本），减少了ODE轨迹的曲率，从而减轻了采样过程中的离散化误差。

   ![image-20250323202911475](./imgs/SiT.png)


<h2 id="5.介绍一下LightningDiT">5.介绍一下LightningDiT</h2>

​	LightningDiT解决了潜在扩散模型中的一个根本性矛盾：高维视觉分词器(tokenizer)改善了重建质量，但会显著降低生成性能。这个"优化困境"(optimization dilemma)使现有系统常常在两者之间做出次优妥协。

## 改进方案

1. 视觉基础模型引导的VAE优化
   - 提出VA-VAE(Vision foundation model Aligned VAE)，通过与预训练视觉基础模型对齐来规范高维潜在空间
   - 引入VF Loss(视觉基础模型对齐损失)，包括边际余弦相似度损失和边际距离矩阵相似度损失
   - 为防止过度正则化，在相似度损失中使用边际(margin)机制
   - 通过自适应权重机制平衡不同损失的贡献
2. 扩散模型训练策略优化
   - 计算层面：使用更大批量、调整优化器超参数
   - 扩散优化：整合整流流(Rectified Flow)、对数正态分布采样、速度方向损失
   - 并行训练：实现多节点训练以加速实验验证
3. 架构改进
   - 采用现代Transformer优化：RMSNorm、SwiGLU、旋转位置嵌入(RoPE)
   - 优化patch size策略，确保系统一致性
   - 结合VA-VAE优势，使DiT能更有效地处理高维潜在空间

![image-20250323204405144](./imgs/LightningDiT.png)


## 第二章 VAE核心基础知识高频考点

<h2 id="1.VAE和AE的区别是什么？">1.VAE和AE的区别是什么？</h2

### 一、AE 与 VAE 基本概念

#### 1. 自编码器（AE）

- **结构**：AE 由编码器（Encoder）和解码器（Decoder）两部分组成，用于将高维输入压缩到低维潜在空间再重构回原始输入。
- **损失函数**：仅包含重构误差（如均方误差或二元交叉熵），训练目标是最小化输入与重构输出间的差距。
- **潜在空间**：映射为确定性向量，编码空间未做正则化，难以从未见向量生成合理样本。

#### 2. 变分自编码器（VAE）

- **结构**：在编码器末端输出潜在分布参数（均值 μ 和对数方差 log σ²），并通过重参数化技巧采样得到隐变量 z，再输入解码器。
- **损失函数**：由重构损失与 KL 散度两部分组成，其中 KL 项度量 q(z|x) 与先验 p(z)（标准正态分布）之差，并对潜在分布进行正则化。
- **潜在空间**：学习到的连续正态分布保证任何从先验采样的 z 都能解码出合理样本，具备随机生成与插值能力。

### 二、核心区别对比

| 特性     | AE                                 | VAE                                |
| -------- | ---------------------------------- | ---------------------------------- |
| 编码方式 | 确定性映射：x → z                  | 概率映射：x → (μ, σ)，z ~ N(μ, σ²) |
| 损失函数 | 仅重构损失                         | 重构损失 + KL 散度                 |
| 潜在空间 | 非正则化、可能出现不连续或稀疏分布 | 正则化为连续正态分布，更易插值     |
| 生成能力 | 对未见向量难以生成合理样本         | 可直接从先验分布采样生成新样本     |

#### 三、应用场景与选型建议

- **AE 适用场景**：
  - 特征提取与降维，如 PCA 替代；
  - 信号或图像去噪，通过重构抑制噪声；
  - 当只需稳定重构，无需从先验生成新样本时，AE 简单高效。
- **VAE 适用场景**：
  - 数据生成与图像插值，利用连续潜在空间在属性间平滑过渡；
  - 属性操作与风格迁移，可在潜在空间中对特征进行加减；
  - 异常检测，通过潜在分布概率评估样本是否偏离训练分布

---
