# 目录

- [1.请讲一讲ViT模型的原理及其优缺点？](#1.请讲一讲ViT模型的原理及其优缺点？)
- [2.请讲一讲CLIp模型的原理及其应用场景？](2.请讲一讲CLIp模型的原理及其应用场景？)


<h2 id="1.请讲一讲ViT模型的原理及其优缺点？">1.请讲一讲ViT模型的原理及其优缺点？</h2>

**ViT模型（Vision Transformer）** 是一种将Transformer架构应用于图像识别任务的模型，由Google团队在2020年提出.
ViT模型的提出是为了解决传统卷积神经网络（CNN）在处理图像时的局限性，通过将图像分割成若干个patch，
并将这些patch视为序列数据输入到Transformer中进行处理，从而**实现图像识别**。

**ViT模型的工作原理**

- **图像分割‌**：首先将输入的图像分割成若干个小的patch（类似于将图像切割成小块）。
- **线性嵌入**‌：每个patch通过一个线性层被转换为高维向量。
- **位置编码‌**：为了保持空间信息，给每个patch添加位置编码。 
- **Transformer编码**‌：将处理后的数据输入到Transformer编码器中进行特征提取和分类。

**ViT模型的优缺点**

**‌优点‌：**

- **简单且效果好**‌：ViT模型结构简单，效果显著，尤其是在大规模数据集上表现优异。
- **可扩展性强**‌：随着数据量的增加，ViT的性能会显著提升。

**‌缺点‌：**
- **计算量大**‌：相比于CNN，ViT的计算量更大，尤其是在小数据集上表现较差。
- **缺乏归纳偏差**‌：ViT缺乏CNN的归纳偏差，如平移不变性和局部感受野，这在一定程度上影响了其性能。


<h2 id="2.请讲一讲CLIp模型的原理及其应用场景？">2.请讲一讲CLIp模型的原理及其应用场景？</h2>

CLIP模型是一种多模态模型，能够通过对比学习同时理解图像和文本。‌ 该模型由两个子模块组成：一个文本编码器和一个图像编码器，通过对比学习将图像和文本的特征对齐，从而在同一个向量空间中进行匹配。CLIP的核心思想是将图像和文本编码到同一个向量空间中，这使得它能够进行文本与图像的跨模态检索
