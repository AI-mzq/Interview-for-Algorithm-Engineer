# 目录

## 第一章 可控数字人生成
- [1.2D数字人生成有什么方向?](#1.2D数字人生成有什么方向?)
- [2.如何基于一个图像生成模型扩展到视频?](#2.如何基于一个图像生成模型扩展到视频?)
- [3.人体驱动的方法有哪些?](#3.人体驱动的常用方法有哪些?)
- [4.可控人体生成的目的是什么，如何做到驱动?](#4.可控人体生成的目的是什么，如何做到驱动?)
- [5.如何提升人体驱动生成中脸部的ID相似度?](#5.如何提升人体驱动生成中脸部的ID相似度?)
- [6.Animate-Anyone的模型结构和原理](#6.Animate-Anyone的模型结构和原理)
- [7.ID保持图像生成和换脸的区别](#7.ID保持图像生成和换脸的区别)
- [8.有哪些专注人像生成的预训练模型?](#8.有哪些专注人像生成的预训练模型?)
- [9.介绍一下IP-Adapter](#9.介绍一下IP-Adapter)
- [10.介绍一下InstantID](#10.介绍一下InstantID)
- [11.介绍一下PuLID](#11.介绍一下PuLID)
- [12.介绍一下UniPortrait](#12.介绍一下UniPortrait)
- [13.介绍一下ConsistentID](#13.介绍一下ConsistentID)
- [14.介绍一下PhotoMaker](#14.介绍一下PhotoMaker)


## 第一章 可控数字人生成

<h2 id="1.2D数字人有什么方向">1.2D数字人有什么方向?</h2>

目前，2D数字人生成的方向包括：

1. 可控人体生成
- ‌**人体驱动** 
- **虚拟换衣**

2. 可控人脸生成
- **人脸属性编辑**
- **换脸**
- **目标人脸引导的人脸驱动生成**
- **音频引导的人脸驱动生成**

3. ID保持的人体图像/视频生成
- **视频写真**

<h2 id="2.如何基于一个图像人体或人脸生成模型扩展到视频?">2.如何基于一个图像生成模型扩展到视频?</h2>

  基于GAN的方案构造视频数据集抽帧进行训练即可，无需添加额外的帧间一致性模块，测试时就可以达到不错的帧间稳定性。由于扩散模型方案建模的多样性强，如果直接逐帧进行推理会导致帧间一致性较差，目前常用的解决方式是采用SD1.5或者SDXL基底模型的基础上，第一阶段使用人脸或人体数据集将底模调整到对应的domain，第二阶段插入一个类似AnimateDiff中提出的Motion Module提升帧间一致性。

<h2 id="3.人体驱动的方法有哪些?">3.人体驱动的方法有哪些?</h2>

|                                                              |     T2V model                 |     Pose Condition                   |     Injection Type                                     |     Others                |
|--------------------------------------------------------------|-------------------------------|--------------------------------------|--------------------------------------------------------|---------------------------|
|     Magic Animate                                            |     AnimateDiff               |     DensePose                        |     ReferenceNet+ControlNet                            |     w/o. alignment        |
|     Animate Anyone                                           |     AnimateDiff               |     DWPose                           |     ReferenceNet+Pose Encoder+CLIP                     |     w/o. alignment        |
|     Moore-Animate Anyone (AA unofficial   implementation)    |     AnimateDiff               |     DWPose                           |     ReferenceNet+Pose Encoder+CLIP                     |     w/o. alignment        |
|     MusePose                                                 |     AnimateDiff               |     DWPose                           |     ReferenceNet+Pose Encoder+CLIP                     |     w/. alignment (2d)    |
|     Champ                                                    |     AnimateDiff               |     DensePose/DWPose/Normal/Depth    |     ReferenceNet+Pose Encoder+CLIP                     |     w/. alignment (2d)    |
|     UniAnimate                                               |     AnimateDiff               |     DWPose                           |     Pose Encoder+CLIP                                  |     w/. alignment (2d)    |
|     ViVidPose                                                |     Stable Video Diffusion    |     DWPose/SMPLX-Shape               |     ReferenceNet+Pose   Encoder+CLIP+Face   Encoder    |     w/. alignment (3d)    |


<h2 id="4.可控人体生成的目的是什么，如何做到驱动?">4.可控人体生成的目的是什么，如何做到驱动?</h2>
    
  不管是文本生成、图像生成、视频生成，如果没有具备可控性，AI作为一个工具，本身能够带来的效能的提升就非常的有限。可控人体生成的目的就是希望通过输入一段目标的姿态序列和一张参考人像图片，能够保持参考人像的背景，人物特征的同时，生成其按照目标序列进行运动的人像视频。


<h2 id="5.如何提升人体驱动生成中脸部的ID相似度?">5.如何提升人体驱动生成中脸部的ID相似度?</h2>

  人脸生成，是 AI 生成视频中最难的场景之一。首先是因为人类对人脸本身就很敏感。一个细微的肌肉表情，就能被解读出不同的含义。人们自拍经常要拍几十张相似的照片，才能挑到合适的角度。因此涉及到人脸的一些形变，很容易就会引起我们的注意。在早期的人体驱动工作中，研究者们并没有过多的采用一些额外的模块约束参考人像和生成人像的脸部ID一致性，仅采用ReferenceNet和CLIP Image Encoder来提取了参考人像信息。在此基础上，有几种方式可以提升脸部ID一致性：
  1. 在训练过程中，计算生成人脸和参考人脸的ID Similarity，并加入ID Loss，
  2. 对于参考人像的人脸区域，使用人脸识别网络提取对应的ID信息，在主干网络中注入

<h2 id="6.Animate-Anyone的模型结构和原理">6.Animate-Anyone的模型结构和原理</h2>

  AnimateAnyone是一种能够将角色图像转换为所需姿势序列控制的动画视频的方法，继承了Stable Diffusion模型的网络设计和预训练权重，并在UNet中插入Motion Module以适应多帧输入。为了解决保持外观一致性的挑战，引入了ReferenceNet，专门设计为UNet结构来捕获参考图像的空间细节。

  ![](./imgs/animate_anyone.png)


<h2 id="7.ID保持图像生成和换脸的区别">7.ID保持图像生成和换脸的区别</h2>

  ID保持图像生成和换脸都可以达到生成和参考人脸相似的人体图像。这两者区别在于，ID保持图像生成是在生成过程中保持了参考图像的ID信息，而换脸则是将目标图像的人脸替换为参考图像的人脸。ID保持图像生成的目的是生成一个新的图像，使其在视觉上与参考图像相似，但不是完全相同。而换脸则是将目标图像的人脸替换为参考图像的人脸，使得目标图像的人脸与参考图像的人脸完全一致。其中，换脸还需要保持目标图像的其他信息不变，如头发、衣服等，而ID保持图像生成则不需要保持这些信息。


<h2 id="8.有哪些专注人像生成的预训练模型?">8.有哪些专注人像生成的预训练模型?</h2>

  随着大规模预训练模型的发展，专注人像生成的预训练模型也在不断涌现。目前，一些专注人像生成的预训练模型包括：
  - **CosmicMan**: 一个基于文本的高保真人物图像生成模型，能够产生与文本描述精确对齐的逼真人物图像。CosmicMan在图像质量和文本-图像对齐方面优于现有模型，如Stable Diffusion和Imagen。它在2D和3D人物生成任务中展现了实用性和潜力。

  - **Arc2Face**: 专注于使用人脸识别技术的核心特征来引导图像的生成，从而实现在各种任务中保持人脸身份的一致性。这意味着Arc2Face可以用于创建非常符合特定人物身份特征的人脸图像，为人脸识别、数字娱乐以及安全领域等提供了新的可能性。

    ![](./imgs/arc2face.png)

<h2 id="9.介绍一下IP-Adapter?">9.介绍一下IP-Adapter</h2>

IP-Adapter试图在现有的文本到图像扩散模型中实现图像提示（image prompt）的能力，以便更有效地生成所需的图像，想要解决一下问题：

1. 文本提示的复杂性：使用文本提示生成图像往往需要复杂的提示工程，这使得生成理想图像变得困难。

2. 文本表达的局限性：文本在表达复杂场景或概念时可能不够丰富，这限制了内容创作的灵活性。

3. 现有方法的局限性：直接从预训练模型微调（fine-tuning）以支持图像提示虽然有效，但需要大量的计算资源，且不兼容其他基础模型、文本提示和结构控制。

技术方案：
- 解耦的交叉注意力机制：IP-Adapter为文本特征和图像特征分别设置了独立的交叉注意力层。这种设计允许模型在保持原始文本到图像模型结构不变的情况下，有效地处理和融合图像提示信息。
- 图像编码器：使用预训练的CLIP图像编码器来提取图像提示的特征。CLIP模型通过对比学习在大量图像-文本对上训练，能够很好地理解和表示图像内容。
- 适配器训练：在训练阶段，只有新添加的交叉注意力层的参数会被训练，而预训练的扩散模型保持冻结状态。这使得IP-Adapter可以快速训练并适应不同的模型和应用。
- 通用性和兼容性：IP-Adapter训练完成后，可以轻松地应用于从同一基础模型微调出的自定义模型，并且与现有的结构控制工具（如ControlNet）兼容，从而实现更多样化的图像生成任务。

通过这些设计，IP-Adapter能够在保持模型轻量级的同时，实现与完全微调的图像提示模型相当甚至更好的性能。此外，IP-Adapter的通用性和兼容性使其在多种图像生成任务中具有广泛的应用潜力，也为后续2D数字人生成工作的可控性提供了一个参考。



<h2 id="10.介绍一下InstantID">10.介绍一下InstantID</h2>
InstantID只需输入单个图像即可生成实现 ID 保留生成各类风格的图像。以往这类生成任务都需要多样本，多轮次的训练微调才能得到特定ID模型，比如Textual Inversion、DreamBooth和 LoRA 等方法，其技术方案主要涉及以下要点：
- ID嵌入：使用预训练的面部模型来提取参考面部图像中的身份嵌入，这些嵌入包含了丰富的语义信息，如身份、年龄和性别等。与CLIP图像嵌入相比，这种方法能够提供更强的语义细节和更高的保真度。

- 图像适配器：引入了一个轻量级的适配器模块，该模块具有解耦的交叉注意力机制，允许将图像作为视觉提示与文本提示一起用于图像生成过程。这种设计使得InstantID能够灵活地处理各种风格的图像个性化。

- IdentityNet：设计了一个IdentityNet，它通过整合面部图像、面部关键点图像和文本提示来引导图像生成过程。IdentityNet在生成过程中完全由面部嵌入指导，而不使用任何文本信息，从而确保了面部身份的保留。

- 训练和推理策略：在训练过程中，只优化Image Adapter和IdentityNet的参数，而保持预训练文本到图像扩散模型的参数不变。在推理过程中，InstantID能够通过单步前向传播生成身份保持的图像，无需微调。


<h2 id="11.介绍一下PuLID">11.介绍一下PuLID</h2>

PuLID（Pure and Lightning ID customization），旨在解决文本到图像（Text-to-Image，T2I）生成中的个性化身份（ID）定制问题，在InstantID之后，PuLID使用SDXL作为基底模型取得了惊艳的效果，其关键组成有五个：
- 引入轻量T2I分支：PuLID在标准的扩散去噪训练分支之外引入了一个轻量T2I（Lightning T2I）分支。这个分支利用快速采样方法从纯噪声开始，通过有限且可控的步骤生成高质量的图像。

- 对比对齐损失：为了最小化ID插入对原始模型行为的影响，PuLID构建了一对对比路径，一个仅由文本提示条件，另一个同时由ID和文本提示条件。通过语义上对齐这两个路径的UNet特征，模型学会了如何在不影响原始模型行为的情况下嵌入ID信息。

- 准确的ID损失：由于闪电T2I分支能够生成高质量的x0，PuLID能够在一个更准确的设置中计算ID损失。这与测试阶段的设置相匹配，使得ID损失的优化更直接和有效。

- 全目标优化：PuLID的完整学习目标是结合扩散损失、对比对齐损失和ID损失。只有新引入的MLPs和交叉注意力层中的可学习线性层Kid和Vid会使用这个目标进行优化，其余部分保持冻结。

- 训练过程：PuLID的训练过程分为三个阶段。第一阶段使用传统的扩散损失进行训练；第二阶段引入ID损失以提高ID保真度；第三阶段添加对比对齐损失，使用完整目标进行微调。



<h2 id="12.介绍一下UniPortrait">12.介绍一下UniPortrait</h2>
UniPortrait 能够在保证ID信息的情况下，根据文本提示词来生成带有ID的个性化图片。在多人生成的场景，可以不通过mask以及文本描述词的改变来实现多ID生成：

- ID嵌入模块（ID Embedding Module）：
使用面部识别模型的倒数第二层特征作为基础内在ID特征，以保留与身份相关的空间信息。结合CLIP图像编码器的局部特征和面部识别模型的浅层特征，以增强面部结构表示。通过DropToken和DropPath正则化技术，显式地将面部结构特征与内在ID特征解耦，防止模型过拟合非本质面部细节。

- ID路由模块（ID Routing Module）：
在每个交叉注意力层内集成ID路由模块，为潜在的面部区域分配独特的ID。预测每个空间位置的离散概率分布，并选择最匹配的ID嵌入参与该位置的注意力机制。

- 两阶段训练方案：第一阶段（单身份训练阶段）：只引入ID嵌入模块，专注于单身份图像的训练。第二阶段（多身份微调阶段）：在第一阶段训练完成后，引入ID路由模块，对多身份图像进行微调。

- 路由正则化损失（Routing Regularization Loss）：通过L2损失和Gumbel-softmax技巧，确保所有ID都被路由，并且每个ID只被路由到一个目标面部区域。

- 兼容性与扩展性：UniPortrait设计为即插即用模块，与现有的生成控制工具（如ControlNet和IP-Adapter）兼容，提供了广泛的应用潜力。





<h2 id="13.介绍一下ConsistentID">13.介绍一下ConsistentID</h2>
ConsistentID在个性化面部生成的精度和多样性方面取得了显著的成果，并且在引入更多多模态ID信息的同时，保持了快速的推理速度，下面是这篇文章的主要贡献：

- 多模态面部提示生成器：该组件结合了面部特征、相应的面部描述和整体面部上下文，以增强面部细节的精确度。它使用一个细粒度的多模态特征提取器和一个面部ID特征提取器，从多条件下生成更详细的面部ID特征。

- 面部ID特征提取器：除了细粒度的面部特征输入条件外，该组件还将角色的整体ID信息作为视觉提示注入到ConsistentID中。这依赖于预训练的CLIP图像编码器和专门版本的IP-Adapter模型（IPA-FaceID-Plus）来生成整个图像的面部嵌入。

- ID保持网络：该网络通过面部注意力定位策略来优化，以保持每个面部区域内的ID一致性。这种策略通过在训练期间引入面部分割掩码来获得与增强文本交叉注意力模块学习的注意力分数，从而确保面部特征的注意力与相应的面部区域对齐。

- 训练和推理细节：在训练过程中，只优化面部编码器和整体面部ID特征提取器中的投影模块的参数，同时保持预训练扩散模型的参数冻结。推理过程中采用延迟主题条件，以平衡身份保持和可编辑性。

- 细粒度人类数据集构建：为了提供详细的面部特征和相应的文本提示，作者引入了一个数据集管道来创建FGID数据集，该数据集包含525,258张图像，提供了比现有公共面部数据集更丰富的细粒度ID信息和详细的面部描述。

通过这些步骤，ConsistentID能够在仅使用单个参考图像的情况下，生成具有高度ID一致性和多样化面部细节的个性化面部图像


<h2 id="14.介绍一下PhotoMaker">14.介绍一下PhotoMaker</h2>

根据[程明明老师的讲解](https://zhuanlan.zhihu.com/p/680468694)，定制化的文生图工作中，基于DreamBooth+LoRA的人像定制应用都有三个资源采集和消耗上的痛点：

1. 定制时间慢：由于需要在“测试”（定制）阶段对模型进行微调，理论上消耗的时间往往需要大约15分钟，在一些商业化的APP上，由于资源分配和排队的问题这一定制时间甚至要更久（数小时到数天）。

2. 消耗显存高：如果不进行额外的优化，在SDXL底模上执行定制化过程只能在专业级显卡上进行，这些专业级显卡的昂贵都是出了名的。

3. 对输入素材要求高：以妙鸭举例，它要求用户输入20张以上高质量的人像照片，且每次定制过程都需要重新进行收集。对用户来说非常不友好。

PhotoMaker希望在训练时，我们的输入图像和输出的目标图像都不来源于同一个图像。其次，且送入多个同一ID的图像提取embedding以得到对输出ID的一个全面且统一的表达。这个embedding将它命名为Stacked ID embedding。Stacked ID embedding中存取的每个embedding它们的图像来源可能姿态不同，表情不同以及配饰不同，但ID都是相同的，因此可以隐式的将ID与其他与ID无关的信息解耦，以使其只表征待输出的ID信息。论文的主要贡献在于提出了一种新的个性化图像生成框架，通过创新的堆叠ID嵌入方法，在保持高效率的同时，提供了高质量的生成结果和强大的泛化能力。此外，论文还通过自动化数据构建流程支持了模型的训练，并在多个应用场景中展示了其优越的性能。
