# 目录

## 第一章 可控数字人生成
- [1.2D数字人生成有什么方向?](#1.2D数字人生成有什么方向?)
- [2.如何基于一个图像生成模型扩展到视频?](#2.如何基于一个图像生成模型扩展到视频?)
- [3.人体驱动的方法有哪些?](#3.人体驱动的常用方法有哪些?)
- [4.可控人体生成的目的是什么，如何做到驱动?](#4.可控人体生成的目的是什么，如何做到驱动?)
- [5.如何提升人体驱动生成中脸部的ID相似度?](#5.如何提升人体驱动生成中脸部的ID相似度?)
- [6.Animate-Anyone的模型结构和原理](#6.Animate-Anyone的模型结构和原理)



## 第一章 可控数字人生成

<h2 id="1.2D数字人有什么方向">1.2D数字人有什么方向?</h2>

目前，2D数字人生成的方向包括：

1. 可控人体生成
- ‌**人体驱动** 
- **虚拟换衣**

2. 可控人脸生成
- **人脸属性编辑**
- **换脸**
- **目标人脸引导的人脸驱动生成**
- **音频引导的人脸驱动生成**

3. ID保持的人体图像/视频生成
- **视频写真**

<h2 id="2.如何基于一个图像人体或人脸生成模型扩展到视频?">2.如何基于一个图像生成模型扩展到视频?</h2>

  基于GAN的方案构造视频数据集抽帧进行训练即可，无需添加额外的帧间一致性模块，测试时就可以达到不错的帧间稳定性。由于扩散模型方案建模的多样性强，如果直接逐帧进行推理会导致帧间一致性较差，目前常用的解决方式是采用SD1.5或者SDXL基底模型的基础上，第一阶段使用人脸或人体数据集将底模调整到对应的domain，第二阶段插入一个类似AnimateDiff中提出的Motion Module提升帧间一致性。

<h2 id="3.人体驱动的方法有哪些?">3.人体驱动的方法有哪些?</h2>

|                                                              |     T2V model                 |     Pose Condition                   |     Injection Type                                     |     Others                |
|--------------------------------------------------------------|-------------------------------|--------------------------------------|--------------------------------------------------------|---------------------------|
|     Magic Animate                                            |     AnimateDiff               |     DensePose                        |     ReferenceNet+ControlNet                            |     w/o. alignment        |
|     Animate Anyone                                           |     AnimateDiff               |     DWPose                           |     ReferenceNet+Pose Encoder+CLIP                     |     w/o. alignment        |
|     Moore-Animate Anyone (AA unofficial   implementation)    |     AnimateDiff               |     DWPose                           |     ReferenceNet+Pose Encoder+CLIP                     |     w/o. alignment        |
|     MusePose                                                 |     AnimateDiff               |     DWPose                           |     ReferenceNet+Pose Encoder+CLIP                     |     w/. alignment (2d)    |
|     Champ                                                    |     AnimateDiff               |     DensePose/DWPose/Normal/Depth    |     ReferenceNet+Pose Encoder+CLIP                     |     w/. alignment (2d)    |
|     UniAnimate                                               |     AnimateDiff               |     DWPose                           |     Pose Encoder+CLIP                                  |     w/. alignment (2d)    |
|     ViVidPose                                                |     Stable Video Diffusion    |     DWPose/SMPLX-Shape               |     ReferenceNet+Pose   Encoder+CLIP+Face   Encoder    |     w/. alignment (3d)    |


<h2 id="4.可控人体生成的目的是什么，如何做到驱动?">4.可控人体生成的目的是什么，如何做到驱动?</h2>
    
  不管是文本生成、图像生成、视频生成，如果没有具备可控性，AI作为一个工具，本身能够带来的效能的提升就非常的有限。可控人体生成的目的就是希望通过输入一段目标的姿态序列和一张参考人像图片，能够保持参考人像的背景，人物特征的同时，生成其按照目标序列进行运动的人像视频。


<h2 id="5.如何提升人体驱动生成中脸部的ID相似度?">5.如何提升人体驱动生成中脸部的ID相似度?</h2>

  人脸生成，是 AI 生成视频中最难的场景之一。首先是因为人类对人脸本身就很敏感。一个细微的肌肉表情，就能被解读出不同的含义。人们自拍经常要拍几十张相似的照片，才能挑到合适的角度。因此涉及到人脸的一些形变，很容易就会引起我们的注意。在早期的人体驱动工作中，研究者们并没有过多的采用一些额外的模块约束参考人像和生成人像的脸部ID一致性，仅采用ReferenceNet和CLIP Image Encoder来提取了参考人像信息。在此基础上，有几种方式可以提升脸部ID一致性：
  1. 在训练过程中，计算生成人脸和参考人脸的ID Similarity，并加入ID Loss，
  2. 对于参考人像的人脸区域，使用人脸识别网络提取对应的ID信息，在主干网络中注入

<h2 id="6.Animate-Anyone的模型结构和原理">6.Animate-Anyone的模型结构和原理</h2>

  AnimateAnyone是一种能够将角色图像转换为所需姿势序列控制的动画视频的方法，继承了Stable Diffusion模型的网络设计和预训练权重，并在UNet中插入Motion Module以适应多帧输入。为了解决保持外观一致性的挑战，引入了ReferenceNet，专门设计为UNet结构来捕获参考图像的空间细节。

  ![](./imgs/animate_anyone.png)