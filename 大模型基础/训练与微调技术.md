<h1 id="目录">目录</h1>

- [1.指令微调](#1.指令微调)
	- [1.指令微调数据制作发展路线](#1.指令微调数据制作发展路线)
	- [2.指令微调数据构造有哪些指导原则？](#2.指令微调数据构造有哪些指导原则？)
- [2.LoRA](#2.Lora)
	- [1.简要介绍一下LoRA](#1.简要介绍一下LoRA)
	- [2.简要介绍一下LoRA的问题以及常见的LoRA改进方案](#2.简要介绍一下LoRA的问题以及常见的LoRA改进方案)
    - [3.LoRA训练时的Rank和alpha作用是什么？](#3.LoRA训练时的Rank和alpha作用是什么？)
    - [4.LoRA模型如何初始化](#4.LoRA模型如何初始化)
- [3.训练技巧](#3.训练技巧)
	- [1.怎么解决训练使用float16导致溢出的问题](#1.怎么解决训练使用float16导致溢出的问题)
	- [2.llm训练的时候用float16，还是bfloat16，float32?](#2.llm训练的时候用float16，还是bfloat16，float32?)
	- [3.float16是什么数据类型？](#3.float16是什么数据类型？)
	- [4.bfloat16和float32是什么数据类型？](#4.bfloat16和float32是什么数据类型？)
	- [5.什么是FP8？](#5.什么是FP8？)
	- [6.大模型微调需要考虑哪些方面？](#6.大模型微调需要考虑哪些方面？)



<h2 id='1.指令微调'>1.指令微调</h2>


<h3 id='1.指令微调数据制作发展路线'>1.指令微调数据制作发展路线</h3>

1. **Scaling law**：在指令微调数据较为匮乏的时期，收集更多的数据是提升性能的大力出奇迹办法。

2. **人工和启发式的数据多样性**：在数据量积累到一定规模后，数据混合配比成为新的研究话题。一些研究成果验证了合适的数据配比可以提升性能，但数据配比没有通用的万能钥匙。

3. **基于模型的多样性**：随着LLMs/MLLMs，可以让它们参与到数据生产和筛选流程中，例如用GPT3.5/4/4V生产数据，用其它LLMs作为数据质量筛选器。（GPT4/GPT4V为指令微调领域贡献了太多数据，这可能也是一种OpenAI吧）

4. **数据效率**：有了LLMs/MLLMs的加持，数据量似乎已经不成大问题。因此高质量数据的多样性、难度和复杂程度成为了关注焦点。满足上述要求的数据意味着用高质量的响应近似真实用户提示，LIMA论证了只要数据质量足够高，数据量会是次要因素。因此，需要自动化或半自动方案对数据进行过滤：

    1. 基于自然语言规则过滤；

    2. 用InsTag对指令微调数据打语义或意图的标签，从而做聚类分析；

    3. 利用GPT4等语言模型过滤噪声数据；

    4. 利用模型的loss等反馈数据对模型的影响，例如评估模型对指令的不确定性（Active Instruction Tuning）；

5. **数据治理、责任和其他问题**：开始关注数据商业条款、许可，版权等问题。


<h3 id='2.指令微调数据构造有哪些指导原则？'>2.指令微调数据构造有哪些指导原则？</h3>

1. **多样性**：覆盖尽可能多的数据/能力/响应类型；

2. **高质量**：Less is More，最好由算法工程师人工检查每一条指令微调数据，保证每条数据的高质量，三个臭皮匠抵不过一个诸葛亮；

3. **复杂性**：提高每条数据的信息量；

4. **每种能力的激活不需要太多数据**；

5. **更自由的强指令跟随能力需要较多数据**；

6. **精调各项能力配比**，避免遗忘；



<h2 id='2.LoRA'>2.LoRA</h2>


<h3 id='1.简要介绍一下LoRA'>1.简要介绍一下LoRA</h3>

LoRA全称Low Rank Adaptation，出自论文《LoRA: Low-Rank Adaptation of Large Language Models》。

LoRA的出发点是：预训练模型的参数量太大，而事实上对下游任务的微调所需要的本征维度(Intrinsic Dimension)并不高。

假设预训练参数 $W_0$ ，微调后的参数为 $W_1$ ，参数更新可以表示：

$$W_1 = W_0 + \Delta W$$

在“本征维度较低”假设下，可以将$\Delta W$做低秩分解：

$$W_1 = W_0 + UV$$

其中 $U \in {\mathbb R}^{m \times r}$；$V \in {\mathbb R}^{r \times n}$ 。 $r$ 可以设置得非常小，而后在微调过程中只微调 $UV$ 。
这样需要被微调的参数量就少了很多很多。

在实践中，要保证模型初始为预训练状态以获得一个好的微调起点，例如将$UV$之一做全0初始化，或者在 $W_0$ 中先减去 $UV$ .


<h3 id='2.简要介绍一下LoRA的问题以及常见的LoRA改进方案'>2.简要介绍一下LoRA的问题以及常见的LoRA改进方案</h3>

LoRA的低秩思路显著提升了微调效率，但同时也受到低秩限制，微调性能和全参微调还是存在差距。

**（1）PLoRA**

**PLoRA**的改进思路是通过多阶段累积低秩矩阵来逼近全参微调性能。
PLoRA在每个训练阶段做一次LoRA微调并在阶段结束时将训练得到的LoRA合并到主干参数中，然后重新初始化LoRA状态。

**(2) LoRA+**

**LoRA+**可以为UV矩阵设置不同的学习率。


<h3 id='3.LoRA训练时的Rank和alpha作用是什么？'>3.LoRA训练时的Rank和alpha作用是什么？</h3>
rank 是矩阵的秩，它决定了模型在微调过程中允许模型学习的新特征的维数。通常，rank 选
的越小，意味着模型的新参数越少，从而降低开销。然后，秩越低，LoRA 层捕捉到的表示能力
就越低，这可能会影响模型在新任务重的表现。
alpha 是 LoRA 中的一个缩放系数，它用于调节 LoRA 层的输出对最终模型的影响。在
LoRA 中，LoRA 层的输出会被乘以一个缩放系数 alpha / rank
 。其计算公式为：

$$
HLoRA =
alpha
/rank × (A × B)
$$

其中 HLoRA 是 LoRA 层的输出。alpha 的作用是控制 LoRA 层在微调过程中对整个模型
输出的影响:
• 当 alpha 较大, LoRA 层对模型的影响会加大，LoRA 层的输出将更强烈地改变模型的行
为。这通常在微调任务需要对模型进行显著调整时使用。
• 当 alpha 较小, LoRA 层对模型的影响较小，意味着微调过程中 LoRA 对模型的改动相对
温和。这通常适用于希望仅对模型进行小幅调整的任务。
因此，alpha 是用来平衡 LoRA 层与原始模型输出的系数，确保 LoRA 层的更新不会过度影响模型的输出，保持模型稳定性


<h3 id='4.LoRA模型如何初始化'>4.LoRA模型如何初始化</h3>

在 LORA 模型中，down 层的权重使用标准正态分布初始化，标准差为 1/rank，而 up 层的权重初始化为 0。原因如下：
- 保持模型的初始输出不变：在模型初始化时，不希望 LoRA 层对模型的输出产生大的改变或干扰。将 up 层的权重初始化为零，意味着 LoRA 层在刚开始时对模型的输出不会有任何影响，模型的行为与没有 LoRA 的原始模型一致。
- 渐进式学习 LoRA 参数：LoRA 层的目标是在不破坏模型预训练的情况下，逐步微调这些新插入的层。因此，在开始时，将 up 层初始化为零可以保证 LoRA 仅从预训练的基础模型输出开始，逐步加入对任务相关的影响。down 层的初始化使用正态分布，它的作用是将输入特征进行压缩，这个压缩操作开始时可以是任意的（因此使用正态分布初始化）。但up层的作用是将低秩表示映射回到高维度，如果 up 层初始化为零，则不会在初始时对原始模型的输出产生干扰。这样，网络可以逐渐通过学习适应新的任务，而不是一开始就对模型施加随机的影响。

<h2 id='3.训练技巧'>3.训练技巧</h2>


<h3 id='1.怎么解决训练使用float16导致溢出的问题？'>1.怎么解决训练使用float16导致溢出的问题</h3>

在训练过程中使用 float16 可能会导致数值溢出（overflow）或下溢（underflow），特别是在处理大模型和高动态范围的数据时。为了解决这些问题，可以采取以下几种策略：

### 1. **损失缩放（Loss Scaling）**

损失缩放是一种常见的方法，用于在使用 float16 进行训练时保持数值稳定性。具体步骤如下：

- 在前向传播过程中，将损失值乘以一个缩放因子（例如 1024 或 65536）。

- 在反向传播计算梯度时，将梯度除以同样的缩放因子。

通过这种方式，可以在梯度计算中保持足够的数值范围，减少下溢和溢出的风险。

```python
loss_scale = 1024.0
scaled_loss = loss * loss_scale
scaled_loss.backward()
for param in model.parameters():
    if param.grad is not None:
        param.grad.data /= loss_scale
```

### 2. **混合精度训练（Mixed Precision Training）**

混合精度训练结合使用 float16 和 float32，以兼顾计算效率和数值稳定性。通常使用 NVIDIA 的 Apex 或 PyTorch 的 AMP（Automatic Mixed Precision）工具来实现混合精度训练。

在 PyTorch 中，可以使用 `torch.cuda.amp` 进行混合精度训练：

```python
from torch.cuda.amp import GradScaler, autocast
scaler = GradScaler()
for input, target in data_loader:
    optimizer.zero_grad()
    with autocast():
        output = model(input)
        loss = criterion(output, target) 
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 3. **模型架构调整**

对于特定的模型，可以通过调整架构来减少溢出和下溢问题。例如：

- 在网络中添加适当的正则化层（如 Batch Normalization）来稳定训练。

- 使用更小的初始学习率，并逐渐增加。

- 采用较浅的网络层，减少过深网络带来的数值不稳定性。
- 
### 4. **动态损失缩放**

动态损失缩放是一种自适应调整损失缩放因子的方法，能够根据训练过程中的数值范围动态调整缩放因子。可以通过 NVIDIA Apex 工具中的 `DynamicLossScaler` 类来实现。

```python
from apex import amp
model, optimizer = amp.initialize(model, optimizer, opt_level="O1")
for input, target in data_loader:
    optimizer.zero_grad()
    with amp.scale_loss(loss, optimizer) as scaled_loss:
        scaled_loss.backward()
    optimizer.step()
```

### 5. **监控和调整训练参数**

在训练过程中，持续监控梯度和权重的数值范围，及时调整训练参数（如学习率、损失缩放因子等）以避免数值问题。可以使用工具（如 TensorBoard）来可视化这些信息，并做出相应的调整。

### 总结

通过结合使用损失缩放、混合精度训练、模型架构调整和动态损失缩放等方法，可以有效解决 float16 训练过程中出现的数值溢出问题，提高训练效率和数值稳定性。


<h3 id='2.llm训练的时候用float16，还是bfloat16，float32?'>2.llm训练的时候用float16，还是bfloat16，float32?</h3>

在训练大规模语言模型（LLM）时，使用不同的数值精度（如float16、bfloat16、float32）各有其优缺点，具体选择取决于训练的需求、硬件支持和精度要求。以下是每种数值精度的特点和适用情况：

1. **Float32（单精度浮点数）**：

   - **特点**：32位浮点数，提供较高的精度和动态范围。

   - **优点**：高精度，数值稳定性好，广泛支持。

   - **缺点**：内存和计算需求较大，训练速度较慢。

   - **适用情况**：用于需要高精度的训练场景，尤其是在数值稳定性非常关键的情况下。

2. **Float16（半精度浮点数）**：

   - **特点**：16位浮点数，精度和动态范围较低。

   - **优点**：内存和计算需求低，能够显著加速训练过程，适合在支持混合精度训练的硬件（如NVIDIA的Tensor Cores）上使用。

   - **缺点**：数值稳定性较差，容易出现溢出和下溢，需要额外的措施（如损失缩放）来确保训练稳定性。

   - **适用情况**：适用于资源受限、需要加速训练的场景，但需要注意数值稳定性。

3. **BFloat16（单精度浮点数的变种）**：

   - **特点**：16位浮点数，与float16相比，具有更大的动态范围，但精度略低。

   - **优点**：较大的动态范围使其在处理大模型和复杂计算时更加稳定，不需要复杂的损失缩放技术。兼顾计算效率和数值稳定性。

   - **缺点**：精度不如float32，但在大多数情况下足够使用。

   - **适用情况**：适用于需要平衡训练速度和数值稳定性的场景，尤其在支持bfloat16的硬件（如Google的TPUs）上效果更佳。

### 总结：

- **Float32**：适用于需要高精度和数值稳定性的训练任务。

- **Float16**：适用于希望加速训练过程并能应对数值稳定性挑战的任务，适合使用NVIDIA GPU的混合精度训练。

- **BFloat16**：适用于需要兼顾训练速度和数值稳定性的任务，尤其在使用Google TPU时。
在实际操作中，bfloat16和混合精度（float16和float32结合使用）往往是大模型训练中的最佳选择，因其在计算效率和数值稳定性之间达到了较好的平衡。


<h3 id='3.float16是什么数据类型？'>3.float16是什么数据类型？</h3>

#### 基本概念

**简要介绍**

半精度浮点数，又称float16，FP16，half-precision floating-point，binary16，用16位二进制来表示数据的浮点数。其格式如图所示：

![FP16](imgs/基础知识/0702-float16.png)

**格式详解**

Sign（符号位）：1位。0表示正数，1表示负数。

Exponent（指数位）：5位。对于float16，范围为00001~11110，偏置常数为15，因此可以表示的数据范围为 $2^{1−15}$ ~ $2^{30−15}$ 。

Fraction（尾数位）：10位。为了确保浮点数的表示唯一性，所有浮点数格式规定尾数部分的高位始终为1，这一位在表示中是隐含的，因此尾数实际上有11位的精度。

因此，float16的计算公式如下（同样也可以采用 $1 \times 2^0 + 0(1) \times 2^1 ...$ 的方式对尾数位进行计算）：

$$ (-1)^{sign} \times 2^{exponent-15} \times (1+ \frac{fraction_{2进制到10进制}}{1024})$$

**特殊数值**

最大正数（65504）： $0 \, 11110 \, 1111111111$

最大负数（-65504）： $1 \, 11110 \, 1111111111$

+0：$0 \, 00000 \, 0000000000$

-0：$1 \, 00000 \, 0000000000$

正无穷：$0 \, 11111 \, 0000000000$

负无穷：$1 \, 11111 \, 0000000000$（

**分辨率和最小分辨率**

分辨率：在当前的指数位设置下，每个最小单位（1/1024）所代表的数值大小，即 $2^{exponent-15} \times  \frac{1}{1024}$ 。因此，不同分辨率的数据相加可能会由于精度丢失而导致小数部分被舍去。

最小分辨率：表示float16，可以表示的最小精度，即 $2^{-14} \times  \frac{1}{1024} \approx 0.000000059604645$


<h3 id='4.bfloat16和float32是什么数据类型？'>4.bfloat16和float32是什么数据类型？</h3>

#### bfloat16

bfloat16，又称BP16，brain floating point，用16位二进制来表示数据的浮点数。相较于float16，其格式是Sign（符号位）为1位；Exponent（指数位）为8位，偏置常数为127；Fraction（位数）为7位。其计算公式，特殊数值，分辨率和最小分辨率的表示方法，同float16。bfloat16的格式如图所示：

![BP16](imgs/基础知识/0702-bfloat16.png)

#### float32

单精度浮点数，又称float32，FP32，single-precision floating-point，binary32，用32位二进制来表示数据的浮点数。相较于float16，其格式是Sign（符号位）为1位；Exponent（指数位）为8位，偏置常数为127；Fraction（位数）为23位。其计算公式，特殊数值，分辨率和最小分辨率的表示方法，同float16。float32的格式如图所示：

![FP32](imgs/基础知识/0702-float32.png)

#### 不常见的数据类型

![FP32](imgs/基础知识/0702-others.png)


<h3 id='5.什么是FP8？'>5.什么是FP8？</h3>

#### 基本概念

FP8是FP16的衍生产物，一般包含1E5M2和1E3M3两种编码格式。其中，E表示指数位，M表示尾数位（也可以用fraction）。

![FP8](imgs/基础知识/0706-FP8.png)

#### 表示范围

FP8也采用浮点表示法（符号位：0正1负；指数位和尾数位共同表示数值大小）。详细请参照FP16。

**差异**：E5M3严格遵守IEEE754标准（指数位全为1，尾数位任意，则表示无穷）。但E4M3不完全遵守，指数位全为1，也需按照浮点数表示法进行有效数字的换算；当且仅当指数和位数全为1时，才表示无穷。

#### 与INT8比较

- FP8 具有更大的表示范围；但在一定范围内，其表示精度相较 INT8 更差。

- 选择合适的缩放因子时，INT8的量化精度高于FP8，且两者之间的误差几乎相差一个数量级。FP8将提供更好的宽容性，对scale的选择不敏感。


<h1 id='6.大模型微调需要考虑哪些方面？'> 6.大模型微调需要考虑哪些方面？</h1>

- **目标领域**：希望改进当前预训练模型的哪些能力？

- **代替方案**：是否有其他替代方案能够达到预期的效果？如果不能是否已经通过替代方案创建了比较的基线？

- **成本**：是否估算了微调的成本（算力，数据等）？