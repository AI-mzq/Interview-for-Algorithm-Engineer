# 目录

## 第一章 Llama Factory 微调

- [1.什么是llama-factory？](#1.什么是llama-factory？)
- [2.怎么下载模型文件？](#2.怎么下载模型文件？)
- [3.怎么准备训练数据集？](#3.怎么准备训练数据集？)
---

## 第一章 Langraph基础知识

<h2 id="1.什么是llama-factory？">1.什么是llama-factory？</h2>

#### 1.基础定义与核心功能：

**（1） 基础定义**

[**LLaMA Factory**](https://llamafactory.readthedocs.io/zh-cn/latest/index.html) 是一个专为大型语言模型（LLM）设计的高效、易用的**训练与微调平台**。其核心目标是通过简化的流程，让用户无需编写代码即可在本地完成多种模型的微调与训练任务，支持丰富的模型类型、训练方法和优化技术。

**（2）核心功能**

- **模型种类**：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。

- **训练算法**：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。

- **运算精度**：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA 微调。

- **优化算法**：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ 和 PiSSA。

- **加速算子**：FlashAttention-2 和 Unsloth。

- **推理引擎**：Transformers 和 vLLM。

- **实验监控**：LlamaBoard、TensorBoard、Wandb、MLflow、SwanLab 等等。

**（3）LLaMA Factory安装**

- **硬件环境校验**：

安装[显卡驱动](https://www.nvidia.cn/Download/index.aspx?lang=c)和[CUDA](https://developer.nvidia.com/cuda-12-2-0-download-archive/)，并使用**nvidia-smi**命令检验。

- **软件环境准备**：

安装[conda](https://www.anaconda.com/download)或者[miniconda](https://www.anaconda.com/docs/getting-started/miniconda/main)，并创建虚拟环境。

```bash
# 创建名为 llama_factory 的 Python 3.10 虚拟环境
conda create -n llama_factory python=3.10

# 激活虚拟环境
conda activate llama_factory

# 安装 PyTorch 2.3.1 + CUDA 12.1 版本（确保显卡驱动支持 CUDA 12.1）
conda install pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch cuda=12.1 -c pytorch -c nvidia
```

拉取[LLaMA Factory代码](https://github.com/hiyouga/LLaMA-Factory.git)并安装。

```bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factorypip install -e ".[torch,metrics]"
```

安装[模型量化](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html)的所需资源。

```bash
# QLoRA
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl

# awq量化
pip install autoawq
```

**启动LLaMA Factory**：

```bash
# 命令行目录查看
llamafactory-cli train -h

# Web唤醒 or CUDA_VISIBLE_DEVICES=0 llamafactory-cli web
llamafactory-cli webui
```


<h2 id="2.怎么下载模型文件？">2.怎么下载模型文件？</h2>

#### 1.手动下载模型：

通过[Hugging Face](https://huggingface.co/models)下载或者魔搭社区[ModelScope](https://modelscope.cn/?from=baidu_sem)下载。以Meta-Llama3-8B-Instruct为例。

```bash
# Hugging Face
git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct

# Model Scope
git clone https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B-Instruct.git
```

#### 2.代码下载模型：

（1）Hugging Face更多的[下载方式](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)

```bash
# Hugging Face下载
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
```

（2）魔搭社区更多的[下载方式](https://www.modelscope.cn/docs/models/download)

```bash
# 魔搭社区下载
from modelscope import snapshot_download
local_dir = ""
model_dir = snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct',local_dir=local_dir)
```

#### 3.模型验证：

（1）更多的模型推理方式：[魔搭社区](https://www.modelscope.cn/models/LLM-Research/Meta-Llama-3-8B-Instruct)，[Hugging Face](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)

```bash
import transformers
import torch

model_id = "LLM-Research/Meta-Llama-3-8B-Instruct"

# 通过模型是否能正常推理验证模型是否下载成功。
pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = pipeline(
    messages,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
print(outputs[0]["generated_text"][-1])
```


<h2 id="3.怎么准备训练数据集？">3.怎么准备训练数据集？</h2>

#### 1.训练数据集的格式：

（1）（增量）预训练数据集

```bash
# Alpaca格式
[
  {"text": "预训练文本"},
  {"text": "预训练文本"}
]
```

（2）监督微调数据集

- **Alpaca格式**：在模型微调时，instruction对应的内容会与input对应的内容拼接后作为人类指令，而output对应的内容作为模型回答。如有，system对应的内容为系统提示词，而history分别代表历史消息中每轮对话的指令和回答。

```bash
# Alpaca格式
[
  {"instruction": "人类指令（必填）"，
  "input": "人类输入（必填）"，
  "output": "模型回答（必填）"，
  "system": "系统提示词（选填）"，
  "history": [
  	["第一轮指令（选填）", "第一轮回答（选填）"],
  	["第二轮指令（选填）", "第二轮回答（选填）"]]
  }
]
```

- **sharegpt格式**：sharegpt格式支持更多的角色种类，比如human，gpt，observation，function等。其中human和observation必须出现在奇数位置，gpt和function必须出现在偶数位置。

```bash
[
	{
	"conversations":[
		{
		"from": "human",
		"value": "人类指令"，
		}，
		{
		"from": "function_call",
		"value": "工具参数"，
		}，
        {
		"from": "observation",
		"value": "工具结果"，
		}，
		{
		"from": "gpt",
		"value": "模型回答"，
		}，
	],
	"system": "系统提示词（选填）",
	"tools": "工具描述（选填）"
	}
]
```

（3）偏好数据

- **Alpaca格式**：

```bash
[
  {
    "instruction": "人类指令（必填）",
    "input": "人类输入（选填）",
    "chosen": "优质回答（必填）",
    "rejected": "劣质回答（必填）"
  }
]
```

- **sharegpt格式**

```bash
{
  "conversations": [
    {
      "from": "human",
      "value": "人类指令"
    }
  ],
  "chosen": {
    "from": "gpt",
    "value": "模型回答！"
  },
  "rejected": {
    "from": "gpt",
    "value": "模型回答"
  }
}
```

#### 2.训练数据集的配置文件：

LLaMA Factory中的文件中包含了所有可用的数据集。如果使用自定义数据集，需要在dataset_info.json文件中添加数据集的描述。 dataset_info.json文件位于LLaMA Factory根目录的data文件下，即**LLaMA-Factory\data**。



（1）（增量）预训练数据集

```bash
"数据集名称": {
  "file_name": "data.json",
  "columns": {
    "prompt": "text"
  }
}
```

（2）监督微调数据集

- **Alpaca格式**：

```bash
"数据集名称": {
  "file_name": "data.json",
  "columns": {
    "prompt": "instruction",
    "query": "input",
    "response": "output",
    "system": "system",
    "history": "history"
  }
}
```

- **sharegpt格式**

```bash
"数据集名称": {
  "file_name": "data.json",
  "formatting": "sharegpt",
  "columns": {
    "messages": "conversations",
    "system": "system",
    "tools": "tools"
  }
}
```

（3）偏好微调

- **Alpaca格式**：

```bash
"数据集名称": {
  "file_name": "data.json",
  "ranking": true,
  "columns": {
    "prompt": "instruction",
    "query": "input",
    "chosen": "chosen",
    "rejected": "rejected"
  }
}
```

- **sharegpt格式**

```bash
"数据集名称": {
  "file_name": "data.json",
  "formatting": "sharegpt",
  "ranking": true,
  "columns": {
    "messages": "conversations",
    "chosen": "chosen",
    "rejected": "rejected"
  }
}
```

一般只需要修改**数据集名称**和**file_name**，其他参数为默认，可以写明。更多的[训练数据格式和配置文件](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html#id4)
